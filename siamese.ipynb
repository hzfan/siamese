{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.parameter import ParameterDict\n",
    "from mxnet import init\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = 'data/train.csv'\n",
    "TEST_CSV = 'data/test-mini.csv'\n",
    "MODEL_SAVING_DIR = 'model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "mx.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Load training and test set\n",
    "train_df = pd.read_csv(TRAIN_CSV, dtype={\"id\": int, \"qid1\": int, \"qid2\": int,\n",
    "                                         \"question1\": str, \"question2\": str, \"is_duplicate\": int})\n",
    "test_df = pd.read_csv(TEST_CSV, dtype={\"test_id\": int,\"question1\": str, \"question2\": str})\n",
    "# train_df = pd.read_csv(TRAIN_CSV)\n",
    "# test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# Prepare embedding\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "\n",
    "questions_cols = ['question1', 'question2']\n",
    "\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "for dataset in [train_df, test_df]:\n",
    "    for index, row in tqdm(dataset.iterrows(), desc=(\"train\" if dataset is train_df else \"test\")):\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "\n",
    "            q2n = []  # q2n -> question numbers representation\n",
    "            for word in text_to_word_list(row[question]):\n",
    "\n",
    "                # Check for unwanted words\n",
    "                if word in stops:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions as word to question as number representation\n",
    "            dataset.at[index, question] = q2n\n",
    "\n",
    "embedding_dim = 128\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad the sequences to maxlen.\n",
    "#if sentences is greater than maxlen, truncates the sentences\n",
    "#if sentences is less the 500, pads with value 0 (most commonly occurrning word)\n",
    "def pad_sequences(sentences,maxlen=500,value=0):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by maxlen.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    padded_sentences = []\n",
    "    for sen in sentences:\n",
    "        new_sentence = []\n",
    "        if(len(sen) > maxlen):\n",
    "            new_sentence = sen[:maxlen]\n",
    "            padded_sentences.append(new_sentence)\n",
    "        else:\n",
    "            num_padding = maxlen - len(sen)\n",
    "            new_sentence = np.append(sen,[value] * num_padding)\n",
    "            padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n",
    "                     train_df.question2.map(lambda x: len(x)).max(),\n",
    "                     test_df.question1.map(lambda x: len(x)).max(),\n",
    "                     test_df.question2.map(lambda x: len(x)).max())\n",
    "\n",
    "# Split to train validation\n",
    "validation_size = int(0.1 * len(train_df))\n",
    "training_size = len(train_df) - validation_size\n",
    "\n",
    "X = train_df[questions_cols]\n",
    "Y = train_df['is_duplicate']\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
    "\n",
    "# Split to dicts\n",
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
    "X_test = {'left': test_df.question1, 'right': test_df.question2}\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# # Make sure everything is ok\n",
    "assert len(X_train['left']) == len(X_train['right'])\n",
    "assert len(X_train['left']) == len(Y_train)\n",
    "\n",
    "# X_train['left']/X_train['right'] is a list of str (m, l)\n",
    "# Y_train is numpy ndarray (m,)\n",
    "\n",
    "Y_net_train = {'label' : Y_train}\n",
    "Y_net_validation = {'label' : Y_validation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 2 ** 23 // (len(vocabulary) + 1)\n",
    "vocabulary_size = scale * (len(vocabulary) + 1)\n",
    "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
    "    dataset[side] = np.array(dataset[side], dtype='int') * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(gluon.HybridBlock):\n",
    "    def __init__(self, embedding_dim, **kwargs):\n",
    "        super(Siamese, self).__init__(**kwargs)\n",
    "        self.encoder = gluon.rnn.LSTM(50,\n",
    "                                      bidirectional=True, input_size=embedding_dim)\n",
    "        self.dropout = gluon.nn.Dropout(0.3)\n",
    "        self.dense = gluon.nn.Dense(32, activation=\"relu\")\n",
    "     \n",
    "    def hybrid_forward(self, F, input0, input1):\n",
    "        out0emb = input0\n",
    "        out0 = self.encoder(out0emb)\n",
    "        out1emb = input1\n",
    "        out1 = self.encoder(out1emb)\n",
    "        out0 = self.dense(self.dropout(out0))\n",
    "        out1 = self.dense(self.dropout(out1))\n",
    "        batchsize = out1.shape[0]\n",
    "        xx = out0.reshape(batchsize, -1)\n",
    "        yy = out1.reshape(batchsize, -1)\n",
    "        manhattan_dis = F.exp(-F.sum(F.abs(xx - yy), axis=1, keepdims = True)) + 0.0001\n",
    "        return manhattan_dis\n",
    "\n",
    "\n",
    "class Embedding(gluon.HybridBlock):\n",
    "    def __init__(self, input_dim, embedding_dim, **kwargs):\n",
    "        super(Embedding, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "    \n",
    "    def hybrid_forward(self, F, input):\n",
    "        emb = self.embedding(input)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class EmbeddingInit(init.Initializer):\n",
    "    def __init__(self, data):\n",
    "        super(EmbeddingInit, self).__init__()\n",
    "        self._data = data\n",
    "    def _init_weight(self, name, data):\n",
    "        data[:] = 0\n",
    "        data[0::scale] = self._data\n",
    "\n",
    "\n",
    "# check the gpus\n",
    "ctx = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3)]\n",
    "print(ctx)\n",
    "\n",
    "# initialize the networknet\n",
    "mx.random.seed(SEED)\n",
    "input_dim = scale * (len(vocabulary) + 1)\n",
    "net1 = {c: Embedding(input_dim, embedding_dim // len(ctx)) for c in ctx}\n",
    "net2 = Siamese(embedding_dim)\n",
    "subembeddings = [mx.nd.array(x) for x in np.split(embeddings, len(net1), axis=1)]\n",
    "for i, (k, v) in enumerate(net1.items()):\n",
    "    v.initialize(init=EmbeddingInit(subembeddings[i]), ctx=k)\n",
    "net2.initialize(init=init.Normal(sigma=0.01), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1 = {k: gluon.Trainer(v.collect_params(), \n",
    "                             'adagrad',\n",
    "                             {'clip_gradient': 1.25}) for (k, v) in net1.items()}\n",
    "trainer2 = gluon.Trainer(net2.collect_params(),\n",
    "                         'adagrad',\n",
    "                         {'clip_gradient': 1.25})\n",
    "loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataiter, epoch):\n",
    "    train_loss = 0\n",
    "    total_size = 0\n",
    "    for i, batch in enumerate(dataiter):\n",
    "        with mx.autograd.record():\n",
    "            # iterate over the left and right question\n",
    "            embs = []\n",
    "            data_lists = []\n",
    "            for k in range(2):\n",
    "                embedding = [net1[c](batch.data[k].as_in_context(c)) for c in ctx]\n",
    "                embs.append(embedding)\n",
    "                # data_list[i][j] is the ith part of embedding of sub-batch j (on gpu(j))\n",
    "                # data_list[i][j] is of shape (B / len(ctx), embedding_dim / len(ctx))\n",
    "                data_list = [gluon.utils.split_and_load(e, ctx, even_split=True) for e in embedding]\n",
    "                data_list = [mx.nd.concat(*[subemb[j] for subemb in data_list], dim=2) for j in range(len(ctx))]\n",
    "                data_lists.append(data_list)\n",
    "            data_list1, data_list2 = data_lists[0], data_lists[1]\n",
    "            label_list = gluon.utils.split_and_load(batch.label[0], ctx, even_split=True)\n",
    "            losses = [loss(net2(X1, X2), Y) for X1, X2, Y in zip(data_list1, data_list2, label_list)] \n",
    "\n",
    "        for i, l in enumerate(losses):\n",
    "            l.backward(retain_graph=True)\n",
    "            for k, v in trainer1.items():\n",
    "                v.step(batch.data[0].shape[0])\n",
    "        trainer2.step(batch.data[0].shape[0])\n",
    "        total_size += batch.data[0].shape[0]\n",
    "        train_loss += sum([l.sum().asscalar() for l in losses])\n",
    "    mx.nd.waitall()\n",
    "    return train_loss / total_size\n",
    "\n",
    "\n",
    "seed = 0\n",
    "mx.random.seed(seed)\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "BATCH_SIZE = 1000\n",
    "LEARNING_R = 0.001\n",
    "EPOCHS = 10\n",
    "THRESHOLD = 0.5\n",
    "dataiter = mx.io.NDArrayIter(X_train, Y_net_train, BATCH_SIZE, True, last_batch_handle='discard')\n",
    "valdataiter = mx.io.NDArrayIter(X_validation, Y_net_validation, BATCH_SIZE, True, last_batch_handle='discard')\n",
    "accuracy_lst = []\n",
    "for epoch in range(EPOCHS):\n",
    "    dataiter.reset()\n",
    "    valdataiter.reset()\n",
    "    train_loss = train_model(dataiter, epoch)\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(valdataiter):\n",
    "    test_loss = 0.\n",
    "    total_size = 0\n",
    "    auc_scores = []\n",
    "    auc_labels = []\n",
    "    for batch in valdataiter:\n",
    "        # Do forward pass on a batch of validation data\n",
    "        data_lists = []\n",
    "        for k in range(2):\n",
    "            embedding = [net1[c](batch.data[k].as_in_context(c)) for c in ctx]\n",
    "            # data_list[i][j] is the ith part of embedding of sub-batch j (on gpu(j))\n",
    "            # data_list[i][j] is of shape (B / len(ctx), embedding_dim / len(ctx))\n",
    "            data_list = [gluon.utils.split_and_load(e, ctx, even_split=False) for e in embedding]\n",
    "            data_list = [mx.nd.concat(*[subemb[j] for subemb in data_list], dim=2) for j in range(len(ctx))]\n",
    "            data_lists.append(data_list)\n",
    "        data_list1, data_list2 = data_lists[0], data_lists[1]\n",
    "        labels = gluon.utils.split_and_load(batch.label[0], ctx, even_split=False)\n",
    "        scores = [net2(X1, X2) for X1, X2 in zip(data_list1, data_list2)]\n",
    "        pys = [loss(s, Y) for s, Y in zip(scores, labels)]\n",
    "        test_loss += sum([l.sum().asscalar() for l in pys])\n",
    "        total_size += batch.data[0].shape[0]\n",
    "        # batch.label[0] is ndarray of shape (B,)\n",
    "        # scores is a list of scores in different gpus\n",
    "        auc_scores.extend([float(item.asscalar()) for score in scores for item in list(score)])\n",
    "        auc_labels.extend([int(item.asscalar())   for label in labels for item in list(label)])\n",
    "    auc = roc_auc_score(auc_labels, auc_scores)\n",
    "    return test_loss / total_size, auc\n",
    "\n",
    "valdataiter.reset()\n",
    "val_loss, auc = validate_model(valdataiter)\n",
    "print(\"{:>12} = {}\".format(\"val_loss\", val_loss))\n",
    "print(\"{:>12} = {}\".format(\"auc\", auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
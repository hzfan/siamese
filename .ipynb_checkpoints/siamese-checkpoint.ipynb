{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nothing\n",
    "import again\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.parameter import ParameterDict\n",
    "from mxnet import init\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = 'data/train-mini.csv'\n",
    "TEST_CSV = 'data/test-mini.csv'\n",
    "MODEL_SAVING_DIR = 'model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 29997it [00:08, 3464.52it/s]\n",
      "test: 9999it [00:02, 3509.38it/s]\n"
     ]
    }
   ],
   "source": [
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Load training and test set\n",
    "train_df = pd.read_csv(TRAIN_CSV, dtype={\"id\": int, \"qid1\": int, \"qid2\": int,\n",
    "                                         \"question1\": str, \"question2\": str, \"is_duplicate\": int})\n",
    "test_df = pd.read_csv(TEST_CSV, dtype={\"test_id\": int,\"question1\": str, \"question2\": str})\n",
    "# train_df = pd.read_csv(TRAIN_CSV)\n",
    "# test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# Prepare embedding\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "\n",
    "questions_cols = ['question1', 'question2']\n",
    "\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "for dataset in [train_df, test_df]:\n",
    "    for index, row in tqdm(dataset.iterrows(), desc=(\"train\" if dataset is train_df else \"test\")):\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "\n",
    "            q2n = []  # q2n -> question numbers representation\n",
    "            for word in text_to_word_list(row[question]):\n",
    "\n",
    "                # Check for unwanted words\n",
    "                if word in stops:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions as word to question as number representation\n",
    "            dataset.at[index, question] = q2n\n",
    "\n",
    "embedding_dim = 128\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'>\n",
      "(26998,)\n"
     ]
    }
   ],
   "source": [
    "#Pad the sequences to maxlen.\n",
    "#if sentences is greater than maxlen, truncates the sentences\n",
    "#if sentences is less the 500, pads with value 0 (most commonly occurrning word)\n",
    "def pad_sequences(sentences,maxlen=500,value=0):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by maxlen.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    padded_sentences = []\n",
    "    for sen in sentences:\n",
    "        new_sentence = []\n",
    "        if(len(sen) > maxlen):\n",
    "            new_sentence = sen[:maxlen]\n",
    "            padded_sentences.append(new_sentence)\n",
    "        else:\n",
    "            num_padding = maxlen - len(sen)\n",
    "            new_sentence = np.append(sen,[value] * num_padding)\n",
    "            padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n",
    "                     train_df.question2.map(lambda x: len(x)).max(),\n",
    "                     test_df.question1.map(lambda x: len(x)).max(),\n",
    "                     test_df.question2.map(lambda x: len(x)).max())\n",
    "\n",
    "# Split to train validation\n",
    "validation_size = int(0.1 * len(train_df))\n",
    "training_size = len(train_df) - validation_size\n",
    "\n",
    "X = train_df[questions_cols]\n",
    "Y = train_df['is_duplicate']\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
    "\n",
    "print(type(X_train.question1))\n",
    "print(type(Y_train))\n",
    "\n",
    "# Split to dicts\n",
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
    "X_test = {'left': test_df.question1, 'right': test_df.question2}\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "print(type(Y_train))\n",
    "print(Y_train.shape)\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# # Make sure everything is ok\n",
    "assert len(X_train['left']) == len(X_train['right'])\n",
    "assert len(X_train['left']) == len(Y_train)\n",
    "\n",
    "# X_train['left']/X_train['right'] is a list of str (m, l)\n",
    "# Y_train is numpy ndarray (m,)\n",
    "\n",
    "Y_net_train = {'label' : Y_train}\n",
    "Y_net_validation = {'label' : Y_validation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpu(0), gpu(1), gpu(2), gpu(3)]\n"
     ]
    }
   ],
   "source": [
    "class Siamese(gluon.HybridBlock):\n",
    "    def __init__(self, embedding_dim, **kwargs):\n",
    "        super(Siamese, self).__init__(**kwargs)\n",
    "        self.encoder = gluon.rnn.LSTM(50,\n",
    "                                      bidirectional=True, input_size=embedding_dim)\n",
    "        self.dropout = gluon.nn.Dropout(0.3)\n",
    "        self.dense = gluon.nn.Dense(32, activation=\"relu\")\n",
    "     \n",
    "    def hybrid_forward(self, F, input0, input1):\n",
    "        out0emb = input0\n",
    "        out0 = self.encoder(out0emb)\n",
    "        out1emb = input1\n",
    "        out1 = self.encoder(out1emb)\n",
    "        out0 = self.dense(self.dropout(out0))\n",
    "        out1 = self.dense(self.dropout(out1))\n",
    "        batchsize = out1.shape[0]\n",
    "        xx = out0.reshape(batchsize, -1)\n",
    "        yy = out1.reshape(batchsize, -1)\n",
    "        manhattan_dis = F.exp(-F.sum(F.abs(xx - yy), axis=1, keepdims = True)) + 0.0001\n",
    "        return manhattan_dis\n",
    "\n",
    "\n",
    "class Embedding(gluon.HybridBlock):\n",
    "    def __init__(self, input_dim, embedding_dim, **kwargs):\n",
    "        super(Embedding, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "    \n",
    "    def hybrid_forward(self, F, input):\n",
    "        emb = self.embedding(input)\n",
    "        return emb\n",
    "\n",
    "\n",
    "#initialize the networknet\n",
    "input_dim = len(vocabulary) + 1\n",
    "ctx = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3)]\n",
    "net1 = {c: Embedding(input_dim, embedding_dim // len(ctx)) for c in ctx}\n",
    "net2 = Siamese(embedding_dim)\n",
    "#check the gpus\n",
    "print(ctx)\n",
    "#initialize the network using the context of GPU\n",
    "for (k, v) in net1.items():\n",
    "    v.initialize(init=init.Normal(sigma=0.01), ctx=k)\n",
    "net2.initialize(init=init.Normal(sigma=0.01), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(30688, 32)\n",
      "(30688, 32)\n",
      "(30688, 32)\n",
      "(30688, 32)\n"
     ]
    }
   ],
   "source": [
    "print(type(embeddings))\n",
    "embeddings = np.split(embeddings, len(net1), axis=1)\n",
    "for i, (k, v) in enumerate(net1.items()):\n",
    "    print(embeddings[i].shape)\n",
    "    gpuembeddings = (mx.nd.array(embeddings[i])).as_in_context(k)\n",
    "    v.embedding.weight.set_data(gpuembeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1 = {k: gluon.Trainer(v.collect_params(), \n",
    "                             'adadelta',\n",
    "                             {'clip_gradient': 1.25}) for (k, v) in net1.items()}\n",
    "trainer2 = gluon.Trainer(net2.collect_params(),\n",
    "                         'adadelta',\n",
    "                         {'clip_gradient': 1.25})\n",
    "loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 105)\n",
      "(1000, 105, 32)\n",
      "(250, 105, 32)\n",
      "(1000, 105)\n",
      "(1000, 105, 32)\n",
      "(250, 105, 32)\n",
      "(250, 420, 32)\n",
      "(250, 420, 32)\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "MXNetError: Shape inconsistent, Provided = [72000], inferred shape=(33600,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-302b41402c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mvaldataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-302b41402c13>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(dataiter, epoch)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mlabel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_and_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#         print(embedding[0].grad.ctx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-302b41402c13>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mlabel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_and_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#         print(embedding[0].grad.ctx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incubator-mxnet/python/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incubator-mxnet/python/mxnet/gluon/block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m   1278\u001b[0m                     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reg_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhybrid_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reg_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-26490a5488e0>\u001b[0m in \u001b[0;36mhybrid_forward\u001b[0;34m(self, F, input0, input1)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhybrid_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mout0emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout0emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mout1emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incubator-mxnet/python/mxnet/gluon/rnn/rnn_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, states, sequence_length, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_RNNLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_RNNLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhybrid_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incubator-mxnet/python/mxnet/gluon/block.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incubator-mxnet/python/mxnet/gluon/block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m   1278\u001b[0m                     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reg_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhybrid_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reg_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incubator-mxnet/python/mxnet/gluon/rnn/rnn_layer.py\u001b[0m in \u001b[0;36mhybrid_forward\u001b[0;34m(self, F, inputs, states, sequence_length, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m                         \"Invalid recurrent state shape. Expecting %s, got %s.\"%(\n\u001b[1;32m    252\u001b[0m                             str(info['shape']), str(state.shape)))\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# out is (output, state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incubator-mxnet/python/mxnet/gluon/rnn/rnn_layer.py\u001b[0m in \u001b[0;36m_forward_kernel\u001b[0;34m(self, F, inputs, states, sequence_length, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                      \u001b[0mlstm_state_clip_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lstm_state_clip_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                      \u001b[0mlstm_state_clip_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lstm_state_clip_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                      lstm_state_clip_nan=self._lstm_state_clip_nan)\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lstm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incubator-mxnet/python/mxnet/ndarray/register.py\u001b[0m in \u001b[0;36mRNN\u001b[0;34m(data, parameters, state, state_cell, sequence_length, state_size, num_layers, bidirectional, mode, p, state_outputs, projection_size, lstm_state_clip_min, lstm_state_clip_max, lstm_state_clip_nan, use_sequence_length, out, name, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/incubator-mxnet/python/mxnet/_ctypes/ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[0;34m(handle, ndargs, keys, vals, out, is_np_op, output_is_list)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mcreate_ndarray_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_global_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_np_ndarray_cls\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_np_op\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_global_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incubator-mxnet/python/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \"\"\"\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: MXNetError: Shape inconsistent, Provided = [72000], inferred shape=(33600,)"
     ]
    }
   ],
   "source": [
    "def train_model(dataiter, epoch):\n",
    "    train_loss = 0\n",
    "    total_size = 0\n",
    "    for i, batch in enumerate(dataiter):\n",
    "        with mx.autograd.record():\n",
    "            # iterate over the left and right question\n",
    "            embs = []\n",
    "            data_lists = []\n",
    "            for k in range(2):\n",
    "                print(batch.data[k].shape)\n",
    "                embedding = [net1[c](batch.data[k].as_in_context(c)) for c in ctx]\n",
    "                print(embedding[0].shape)\n",
    "                embs.append(embedding)\n",
    "                # data_list[i][j] is the ith part of embedding of sub-batch j (on gpu(j))\n",
    "                # data_list[i][j] is of shape (B / len(ctx), embedding_dim / len(ctx))\n",
    "                data_list = [gluon.utils.split_and_load(e, ctx, even_split=True) for e in embedding]\n",
    "                print(data_list[0][0].shape)\n",
    "                data_list = [mx.nd.concat(*[subemb[j] for subemb in data_list], dim=1) for j in range(len(ctx))]\n",
    "                data_lists.append(data_list)\n",
    "            data_list1, data_list2 = data_lists[0], data_lists[1]\n",
    "            print(data_list1[0].shape)\n",
    "            print(data_list2[0].shape)\n",
    "            label_list = gluon.utils.split_and_load(batch.label[0], ctx, even_split=True)\n",
    "            losses = [loss(net2(X1, X2), Y) for X1, X2, Y in zip(data_list1, data_list2, label_list)] \n",
    "\n",
    "#         print(embedding[0].grad.ctx)\n",
    "#         print(embedding[1].grad.ctx)\n",
    "#         print(embedding[0]._fresh_grad)\n",
    "#         print(net1[ctx[0]].embedding.weight._data[0]._fresh_grad)\n",
    "#         print(net2.dense.weight._data[0]._fresh_grad)\n",
    "        for i, l in enumerate(losses):\n",
    "            l.backward(retain_graph=True)\n",
    "            for k, v in trainer1.items():\n",
    "                v.step(batch.data[0].shape[0])\n",
    "#         print(embedding[0]._fresh_grad)\n",
    "#         print(net1[ctx[0]].embedding.weight._data[0]._fresh_grad)\n",
    "#         print(net2.dense.weight._data[0]._fresh_grad)\n",
    "        trainer2.step(batch.data[0].shape[0])\n",
    "        total_size += batch.data[0].shape[0]\n",
    "        train_loss += sum([l.sum().asscalar() for l in losses])\n",
    "    mx.nd.waitall()\n",
    "    return train_loss / total_size\n",
    "\n",
    "\n",
    "seed = 0\n",
    "mx.random.seed(seed)\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "BATCH_SIZE = 1000\n",
    "LEARNING_R = 0.001\n",
    "EPOCHS = 10\n",
    "THRESHOLD = 0.5\n",
    "dataiter = mx.io.NDArrayIter(X_train, Y_net_train, BATCH_SIZE, True, last_batch_handle='discard')\n",
    "valdataiter = mx.io.NDArrayIter(X_validation, Y_net_validation, BATCH_SIZE, True, last_batch_handle='discard')\n",
    "accuracy_lst = []\n",
    "for epoch in range(EPOCHS):\n",
    "    dataiter.reset()\n",
    "    valdataiter.reset()\n",
    "    train_loss = train_model(dataiter, epoch)\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    val_loss = 0.10158449363708497\n",
      "         auc = 0.7184138970193458\n"
     ]
    }
   ],
   "source": [
    "def validate_model(valdataiter):\n",
    "    test_loss = 0.\n",
    "    total_size = 0\n",
    "    auc_scores = []\n",
    "    auc_labels = []\n",
    "    for batch in valdataiter:\n",
    "        # Do forward pass on a batch of validation data\n",
    "        data_lists = []\n",
    "        for k in range(2):\n",
    "            embedding = [net1[c](batch.data[k].as_in_context(c)) for c in ctx]\n",
    "            # data_list[i][j] is the ith part of embedding of sub-batch j (on gpu(j))\n",
    "            # data_list[i][j] is of shape (B / len(ctx), embedding_dim / len(ctx))\n",
    "            data_list = [gluon.utils.split_and_load(e, ctx, even_split=False) for e in embedding]\n",
    "            data_list = [mx.nd.concat(*[subemb[j] for subemb in data_list], dim=1) for j in range(len(ctx))]\n",
    "            data_lists.append(data_list)\n",
    "        data_list1, data_list2 = data_lists[0], data_lists[1]\n",
    "        labels = gluon.utils.split_and_load(batch.label[0], ctx, even_split=False)\n",
    "        scores = [net2(X1, X2) for X1, X2 in zip(data_list1, data_list2)]\n",
    "        pys = [loss(s, Y) for s, Y in zip(scores, labels)]\n",
    "        test_loss += sum([l.sum().asscalar() for l in pys])\n",
    "        total_size += batch.data[0].shape[0]\n",
    "        # batch.label[0] is ndarray of shape (B,)\n",
    "        # scores is a list of scores in different gpus\n",
    "        auc_scores.extend([float(item.asscalar()) for score in scores for item in list(score)])\n",
    "        auc_labels.extend([int(item.asscalar())   for label in labels for item in list(label)])\n",
    "    auc = roc_auc_score(auc_labels, auc_scores)\n",
    "    return test_loss / total_size, auc\n",
    "\n",
    "valdataiter.reset()\n",
    "val_loss, auc = validate_model(valdataiter)\n",
    "print(\"{:>12} = {}\".format(\"val_loss\", val_loss))\n",
    "print(\"{:>12} = {}\".format(\"auc\", auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "[[3. 3.]\n",
      " [3. 3.]\n",
      " [3. 3.]\n",
      " [3. 3.]]\n",
      "<NDArray 4x2 @gpu(0)>\n",
      "\n",
      "[[2. 2.]\n",
      " [2. 2.]\n",
      " [2. 2.]\n",
      " [2. 2.]]\n",
      "<NDArray 4x2 @gpu(1)>\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet.gluon import HybridBlock\n",
    "from mxnet.gluon.utils import split_and_load\n",
    "\n",
    "ctx = [mx.gpu(0), mx.gpu(1)]\n",
    "\n",
    "\n",
    "class Net1(HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.weight = self.params.get('weight', shape=(4, 2),\n",
    "                                      dtype='float32', init=None, allow_deferred_init=False)\n",
    "\n",
    "    def hybrid_forward(self, F, embedding, weight):\n",
    "        return embedding * weight\n",
    "\n",
    "\n",
    "class Net2(HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.weight = self.params.get('weight', shape=(4, 2),\n",
    "                                      dtype='float32', init=None, allow_deferred_init=False)\n",
    "\n",
    "    def hybrid_forward(self, F, embedding, weight):\n",
    "        return embedding * weight\n",
    "\n",
    "net1 = Net1()\n",
    "net2 = Net2()\n",
    "net1.initialize(ctx=[mx.gpu(0)])\n",
    "net2.initialize(ctx=[mx.gpu(1)])\n",
    "net1.weight.set_data(mx.nd.ones((4, 2)) * 2)\n",
    "net2.weight.set_data(mx.nd.ones((4, 2)) * 3)\n",
    "with mx.autograd.record():\n",
    "    embedding = mx.nd.ones((4, 2)).as_in_context(mx.gpu(0))\n",
    "#     embedding.attach_grad()\n",
    "    out1 = net1(embedding)\n",
    "    out2 = net2(out1.as_in_context(mx.gpu(1)))\n",
    "    loss = out2.sum()\n",
    "\n",
    "loss.backward()\n",
    "print(embedding.grad)\n",
    "print(net1.weight._data[0].grad)\n",
    "print(net2.weight._data[0].grad)\n",
    "print(net1.weight._data[0]._fresh_grad)\n",
    "print(net2.weight._data[0]._fresh_grad)\n",
    "\n",
    "# for l in losses:\n",
    "#     print(\"=\" * 64)\n",
    "#     l.backward()\n",
    "#     print(embedding.grad)\n",
    "\n",
    "# with mx.autograd.record():\n",
    "#     a = mx.nd.ones((4, 2))\n",
    "#     b = 2 * a\n",
    "#     b.attach_grad()\n",
    "#     c = b + 1\n",
    "#     loss = c.sum()\n",
    "# loss.backward()\n",
    "# print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "\n",
      "[[6. 6.]\n",
      " [6. 6.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "<NDArray 4x2 @gpu(0)>\n",
      "\n",
      "[[6. 6.]\n",
      " [6. 6.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "<NDArray 4x2 @gpu(1)>\n",
      "\n",
      "[[4. 4.]\n",
      " [4. 4.]]\n",
      "<NDArray 2x2 @gpu(0)>\n",
      "\n",
      "[[2. 2.]\n",
      " [2. 2.]\n",
      " [2. 2.]\n",
      " [2. 2.]]\n",
      "<NDArray 4x2 @gpu(0)>\n",
      "\n",
      "[[2. 2.]\n",
      " [2. 2.]\n",
      " [2. 2.]\n",
      " [2. 2.]]\n",
      "<NDArray 4x2 @gpu(1)>\n",
      "\n",
      "[[3. 3.]\n",
      " [3. 3.]]\n",
      "<NDArray 2x2 @gpu(0)>\n",
      "\n",
      "[[1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [2.   2.  ]\n",
      " [2.   2.  ]]\n",
      "<NDArray 4x2 @gpu(0)>\n",
      "\n",
      "[[1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [2.   2.  ]\n",
      " [2.   2.  ]]\n",
      "<NDArray 4x2 @gpu(1)>\n",
      "\n",
      "[[3. 3.]\n",
      " [3. 3.]]\n",
      "<NDArray 2x2 @gpu(0)>\n",
      "================================================================\n",
      "\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [6. 6.]\n",
      " [6. 6.]]\n",
      "<NDArray 4x2 @gpu(0)>\n",
      "\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [6. 6.]\n",
      " [6. 6.]]\n",
      "<NDArray 4x2 @gpu(1)>\n",
      "\n",
      "[[4. 4.]\n",
      " [4. 4.]]\n",
      "<NDArray 2x2 @gpu(0)>\n",
      "\n",
      "[[1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [2.   2.  ]\n",
      " [2.   2.  ]]\n",
      "<NDArray 4x2 @gpu(0)>\n",
      "\n",
      "[[1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [2.   2.  ]\n",
      " [2.   2.  ]]\n",
      "<NDArray 4x2 @gpu(1)>\n",
      "\n",
      "[[3. 3.]\n",
      " [3. 3.]]\n",
      "<NDArray 2x2 @gpu(0)>\n",
      "\n",
      "[[1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [1.85 1.85]]\n",
      "<NDArray 4x2 @gpu(0)>\n",
      "\n",
      "[[1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [1.85 1.85]]\n",
      "<NDArray 4x2 @gpu(1)>\n",
      "\n",
      "[[3. 3.]\n",
      " [3. 3.]]\n",
      "<NDArray 2x2 @gpu(0)>\n",
      "================================================================\n",
      "\n",
      "[[1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [1.85 1.85]]\n",
      "<NDArray 4x2 @gpu(0)>\n",
      "\n",
      "[[1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [1.85 1.85]\n",
      " [1.85 1.85]]\n",
      "<NDArray 4x2 @gpu(1)>\n",
      "\n",
      "[[2.8 2.8]\n",
      " [2.8 2.8]]\n",
      "<NDArray 2x2 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import HybridBlock\n",
    "from mxnet.gluon.utils import split_and_load\n",
    "\n",
    "ctx = [mx.gpu(0), mx.gpu(1)]\n",
    "\n",
    "\n",
    "class Net1(HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.weight = self.params.get('weight', shape=(4, 2),\n",
    "                                      dtype='float32', init=None, allow_deferred_init=False)\n",
    "\n",
    "    def hybrid_forward(self, F, embedding, weight):\n",
    "        return embedding * weight  # * 2\n",
    "\n",
    "\n",
    "class Net2(HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.weight = self.params.get('weight', shape=(2, 2),\n",
    "                                      dtype='float32', init=None, allow_deferred_init=False)\n",
    "\n",
    "    def hybrid_forward(self, F, embedding, weight):\n",
    "        return embedding * weight  # * 3\n",
    "\n",
    "net1 = [Net1(), Net1()]\n",
    "net2 = Net2()\n",
    "net1[0].initialize(ctx=[mx.gpu(0)])\n",
    "net1[1].initialize(ctx=[mx.gpu(1)])\n",
    "net2.initialize(ctx=[mx.gpu(0), mx.gpu(1)])\n",
    "net1[0].weight.set_data(mx.nd.ones((4, 2)) * 2)\n",
    "net1[1].weight.set_data(mx.nd.ones((4, 2)) * 2)\n",
    "net2.weight.set_data(mx.nd.ones((2, 2)) * 3)\n",
    "\n",
    "trainer1 = [gluon.Trainer(net.collect_params(), \n",
    "                          'sgd') for net in net1]\n",
    "trainer2 = gluon.Trainer(net2.collect_params(),\n",
    "                         'sgd')\n",
    "\n",
    "with mx.autograd.record():\n",
    "    embedding = mx.nd.ones((4, 2)).as_in_context(mx.gpu(0))\n",
    "#     embedding.attach_grad()\n",
    "    out1_0 = net1[0](embedding)\n",
    "    out1_1 = net1[1](out1_0.as_in_context(mx.gpu(1)))\n",
    "#     out1_1.attach_grad()\n",
    "#     losses = [out1_1.sum()]\n",
    "    out2 = [net2(out1_1[0: 2].as_in_context(mx.gpu(0))), net2(out1_1[2:].as_in_context(mx.gpu(1)))]\n",
    "    losses = [out2[0].sum(), out2[1].sum()]\n",
    "\n",
    "for l in losses:\n",
    "    print(\"=\" * 64)\n",
    "    l.backward(retain_graph=True)\n",
    "#     out1_1.backward()\n",
    "#     print(embedding.grad)\n",
    "    print(net1[0].weight._data[0].grad)\n",
    "    print(net1[1].weight._data[0].grad)\n",
    "    print(net2.weight._data[0].grad)\n",
    "#     print(net1[0].weight._data[0]._fresh_grad)\n",
    "#     print(net1[1].weight._data[0]._fresh_grad)\n",
    "#     print(net2.weight._data[0]._fresh_grad)\n",
    "    print(net1[0].weight._data[0])\n",
    "    print(net1[1].weight._data[0])\n",
    "    print(net2.weight._data[0])\n",
    "    trainer1[0].step(4)\n",
    "    trainer1[1].step(4)\n",
    "    print(net1[0].weight._data[0])\n",
    "    print(net1[1].weight._data[0])\n",
    "    print(net2.weight._data[0])\n",
    "print(\"=\" * 64)\n",
    "trainer2.step(4)\n",
    "print(net1[0].weight._data[0])\n",
    "print(net1[1].weight._data[0])\n",
    "print(net2.weight._data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
